{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSimilarity:\n",
    "    \n",
    "    def __init__(self, tags_dict = None, correlation_matrix = None):\n",
    "        \n",
    "        self._tags = self._get_tags_dict() if tags_dict is None else tags_dict\n",
    "        self._no_of_tags = len(self._tags) \n",
    "        self._tag_correlation_matrix = np.identity(self._no_of_tags) if correlation_matrix is None else correlation_matrix\n",
    "        self._parser = spacy.load(\"en\")\n",
    "        \n",
    "    def _get_tags_dict(self):\n",
    "        with open(\"data/tags.txt\",\"r\") as fl:\n",
    "            data = fl.read()\n",
    "\n",
    "        tags = [each.strip(\"\\n\").strip() for each in re.findall(r\"\\n[a-z]*\\s\",data)]\n",
    "        tags.extend([\"acl\",\"ROOT\"])\n",
    "        tags = sorted(tags)\n",
    "        tags = {each[1]:each[0] for each in enumerate(tags,start=0)}\n",
    "        return tags\n",
    "    \n",
    "    def _similarity_word(self, pair_A, pair_B):\n",
    "\n",
    "        #getting head and dependent texts \n",
    "        head_a, head_b = pair_A[0].text, pair_B[0].text\n",
    "        dep_a, dep_b = pair_A[2].text, pair_B[2].text\n",
    "\n",
    "        if head_a == head_b:\n",
    "            head = 1\n",
    "        else:\n",
    "            try:\n",
    "                #WordNet synsets for heads\n",
    "                head_a, head_b = wn.synsets(head_a)[0], wn.synsets(head_b)[0]\n",
    "\n",
    "                #path based similarity (Li et. al) for head\n",
    "                head = head_a.path_similarity(head_b)\n",
    "\n",
    "                head = 0 if head is None else head  \n",
    "\n",
    "            except Exception:\n",
    "                head = 0\n",
    "\n",
    "        if dep_a == dep_b:\n",
    "            dep = 1\n",
    "        else:\n",
    "            try:\n",
    "                #WordNet synsets for dependent\n",
    "                dep_a, dep_b = wn.synsets(dep_a)[0], wn.synsets(dep_b)[0]\n",
    "\n",
    "                #path based similarity (Li et. al) for dependent\n",
    "                dep = dep_a.path_similarity(dep_b)\n",
    "\n",
    "                dep = 0 if dep is None else dep\n",
    "\n",
    "            except Exception:\n",
    "                dep = 0     \n",
    "\n",
    "        return head + dep\n",
    "\n",
    "    def _similarity_tag(self, tag_a, tag_b):\n",
    "        \n",
    "        tag_a_id, tag_b_id = self._tags[tag_a], self._tags[tag_b] \n",
    "        score = self._tag_correlation_matrix[tag_a_id,tag_b_id]\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def semantic_similarity(self, document_1, document_2):\n",
    "\n",
    "        #parsing documets using spaCy English language parser\n",
    "        tokens_1,tokens_2 = self._parser(document_1), self._parser(document_2)\n",
    "\n",
    "        #seperating dependency pairs and tags from tokens\n",
    "        pairs_1 = [(token.head,token.dep_,token) for token in tokens_1]\n",
    "        pairs_2 = [(token.head,token.dep_,token) for token in tokens_2]\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        #calculating score \n",
    "        for pair_A in pairs_1:\n",
    "\n",
    "            for pair_B in pairs_2:\n",
    "\n",
    "                score += self._similarity_word(pair_A, pair_B) * self._similarity_tag(pair_A[1], pair_B[1])\n",
    "\n",
    "        #averaging score \n",
    "        score = score / (len(tokens_1) + len(tokens_2))\n",
    "\n",
    "        return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = TextSimilarity()\n",
    "sim._tag_correlation_matrix = torch.from_numpy(sim._tag_correlation_matrix)\n",
    "sim._tag_correlation_matrix.requires_grad = True\n",
    "score = sim.semantic_similarity(\"This is a boy.\",\"This is a girl.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(score, target):\n",
    "    return target - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_function(score, 0.9)\n",
    "loss.backward()\n",
    "sim._tag_correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model, epochs = 25, lr = 0.001):\n",
    "    \n",
    "    model._tag_correlation_matrix = torch.from_numpy(model._tags)\n",
    "    model._tag_correlation_matrix.requires_grad = True\n",
    "    \n",
    "    criterion = loss_function\n",
    "    optimizer = torch.optim.Adam(model._tags, lr=lr)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
