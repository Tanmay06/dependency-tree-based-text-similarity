{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import MSELoss, Module\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size):\n",
    "    \n",
    "    no_of_batches = len(data) // batch_size\n",
    "    \n",
    "    for n in range(0, len(data), no_of_batches):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            x1 = data.sent_1.iloc[n:n + no_of_batches].values\n",
    "            x2 = data.sent_2.iloc[n:n + no_of_batches].values\n",
    "            Y = data.score.iloc[n:n + no_of_batches].values\n",
    "        \n",
    "        except IndexError:\n",
    "            \n",
    "            x1 = data.sent_1.iloc[n:].values\n",
    "            x2 = data.sent_2.iloc[n:].values\n",
    "            Y = data.score.iloc[n:].values\n",
    "    \n",
    "    yield x1, x2, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSimilarity():\n",
    "    \n",
    "    def __init__(self, tags_dict = None, correlation_matrix = None):\n",
    "        \n",
    "        self._tags = self._get_tags_dict() if tags_dict is None else tags_dict\n",
    "        self._no_of_tags = len(self._tags) \n",
    "        self._tag_correlation_matrix = np.identity(self._no_of_tags) if correlation_matrix is None else correlation_matrix\n",
    "        self._parser = spacy.load(\"en\")\n",
    "        \n",
    "    def _get_tags_dict(self):\n",
    "        \n",
    "        with open(\"data/tags.json\",\"r\") as fl:\n",
    "            tags = json.load(fl)\n",
    "            \n",
    "        return tags\n",
    "    \n",
    "    def _similarity_word(self, pair_A, pair_B):\n",
    "\n",
    "        #getting head and dependent texts \n",
    "        head_a, head_b = pair_A[0].text, pair_B[0].text\n",
    "        dep_a, dep_b = pair_A[2].text, pair_B[2].text\n",
    "\n",
    "        if head_a == head_b:\n",
    "            head = 1\n",
    "        else:\n",
    "            try:\n",
    "                #WordNet synsets for heads\n",
    "                head_a, head_b = wn.synsets(head_a)[0], wn.synsets(head_b)[0]\n",
    "\n",
    "                #path based similarity (Li et. al) for head\n",
    "                head = head_a.path_similarity(head_b)\n",
    "\n",
    "                head = 0 if head is None else head  \n",
    "\n",
    "            except Exception:\n",
    "                head = 0\n",
    "\n",
    "        if dep_a == dep_b:\n",
    "            dep = 1\n",
    "        else:\n",
    "            try:\n",
    "                #WordNet synsets for dependent\n",
    "                dep_a, dep_b = wn.synsets(dep_a)[0], wn.synsets(dep_b)[0]\n",
    "\n",
    "                #path based similarity (Li et. al) for dependent\n",
    "                dep = dep_a.path_similarity(dep_b)\n",
    "\n",
    "                dep = 0 if dep is None else dep\n",
    "\n",
    "            except Exception:\n",
    "                dep = 0     \n",
    "\n",
    "        return head + dep\n",
    "\n",
    "    def _similarity_tag(self, tag_a, tag_b):\n",
    "        \n",
    "        tag_a_id, tag_b_id = self._tags[tag_a], self._tags[tag_b] \n",
    "        score = self._tag_correlation_matrix[tag_a_id,tag_b_id]\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def semantic_similarity(self, documents_1, documents_2):\n",
    "        \n",
    "        #checking the sizes of both documents\n",
    "        assert len(documents_1) == len(documents_2), \"Size of both lists should be same.\"\n",
    "        \n",
    "        #scores vector\n",
    "        scores = torch.zeros([len(documents_1),],dtype=torch.double)\n",
    "        \n",
    "        i = 0\n",
    "            \n",
    "        for document_1, document_2 in zip(documents_1,documents_2):\n",
    "            \n",
    "            #parsing documets using spaCy English language parser\n",
    "            tokens_1,tokens_2 = self._parser(document_1), self._parser(document_2)\n",
    "\n",
    "            #seperating dependency pairs and tags from tokens\n",
    "            pairs_1 = [(token.head,token.dep_,token) for token in tokens_1]\n",
    "            pairs_2 = [(token.head,token.dep_,token) for token in tokens_2]\n",
    "\n",
    "            score = 0\n",
    "\n",
    "            #calculating score \n",
    "            for pair_A in pairs_1:\n",
    "\n",
    "                for pair_B in pairs_2:\n",
    "\n",
    "                    score += self._similarity_word(pair_A, pair_B) * self._similarity_tag(pair_A[1], pair_B[1])\n",
    "\n",
    "            #averaging score \n",
    "            score = score / (len(tokens_1) + len(tokens_2))\n",
    "            \n",
    "            scores[i] += score\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        return scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8611, 0.8667], dtype=torch.float64, grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = TextSimilarity()\n",
    "sim._tag_correlation_matrix = torch.from_numpy(sim._tag_correlation_matrix)\n",
    "sim._tag_correlation_matrix.requires_grad = True\n",
    "sent_1 = [\"he is boy\",\"it is dog\"]\n",
    "sent_2 = [\"he is girl\",\"it is cat\"]\n",
    "score = sim.semantic_similarity(sent_1,sent_2)\n",
    "print(score)\n",
    "sim._tag_correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03706790123456788\n",
      "tensor([[-0.1815,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0010, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MSELoss()\n",
    "optimizer = torch.optim.Adam([sim._tag_correlation_matrix], lr=0.001)\n",
    "target = torch.DoubleTensor([1.,1.])\n",
    "target.requires_grad = True\n",
    "loss_score = loss(score, target)\n",
    "print(loss_score.item())\n",
    "loss_score.backward()\n",
    "print(sim._tag_correlation_matrix.grad)\n",
    "optimizer.step()\n",
    "sim._tag_correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model, epochs = 25, lr = 0.001, validation_thresh = 0.2, batch_size = 10, print_every = 5):\n",
    "    \n",
    "    model._tag_correlation_matrix = torch.from_numpy(model._tag_correlation_matrix)\n",
    "    model._tag_correlation_matrix.requires_grad = True\n",
    "    \n",
    "    criterion = MSELoss()\n",
    "    optimizer = torch.optim.Adam([model._tag_correlation_matrix], lr=lr)\n",
    "    \n",
    "    thresh = int(validation_thresh * len(data))\n",
    "    \n",
    "    train_data = data.iloc[:thresh]\n",
    "    valid_data = data.iloc[thresh:]\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    count = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for x1, x2, Y in get_batches(train_data, batch_size):\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "            scores = model.semantic_similarity(x1, x2)\n",
    "            \n",
    "            Y = torch.DoubleTensor(Y)\n",
    "            \n",
    "            loss = criterion(scores, Y)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            if count % print_every == 0:\n",
    "                \n",
    "                train_losses.append(np.mean(losses))\n",
    "                losses = []\n",
    "                \n",
    "                for x1, x2, Y in get_batches(valid_data, batch_size):\n",
    "                    \n",
    "                    scores = model.semantic_similarity(x1, x2)\n",
    "                    \n",
    "                    Y = torch.DoubleTensor(Y)\n",
    "                    \n",
    "                    loss = criterion(scores, Y)\n",
    "\n",
    "                    losses.append(loss.item())\n",
    "                    \n",
    "                valid_losses.append(np.mean(losses))\n",
    "                \n",
    "                print(f\"{count} {epoch}/{epochs}\\ttraining loss:{train_losses[-1]}\\tvalididation loss:{valid_losses[-1]}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The problem likely will mean corrective change...</td>\n",
       "      <td>He said the problem needs to be corrected befo...</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The technology-laced Nasdaq Composite Index .I...</td>\n",
       "      <td>The broad Standard &amp; Poor's 500 Index .SPX inc...</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\"It's a huge black eye,\" said publisher Arthur...</td>\n",
       "      <td>\"It's a huge black eye,\" Arthur Sulzberger, th...</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SEC Chairman William Donaldson said there is a...</td>\n",
       "      <td>\"I think there's a building confidence that th...</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Vivendi shares closed 1.9 percent at 15.80 eur...</td>\n",
       "      <td>In New York, Vivendi shares were 1.4 percent d...</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Myanmar's pro-democracy leader Aung San Suu Ky...</td>\n",
       "      <td>Myanmar's pro-democracy leader Aung San Suu Ky...</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Darren Dopp, a Spitzer spokesman, declined to ...</td>\n",
       "      <td>John Heine, a spokesman for the commission in ...</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Bremer said one initiative is to launch a US$7...</td>\n",
       "      <td>Bremer said he would launch a $70-million prog...</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>In afternoon trading in Europe, France's CAC-4...</td>\n",
       "      <td>In Europe, France's CAC-40 rose 1.3 percent, B...</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>He later learned that the incident was caused ...</td>\n",
       "      <td>He later found out the alarming incident had b...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             sent_1  \\\n",
       "0           0  The problem likely will mean corrective change...   \n",
       "1           1  The technology-laced Nasdaq Composite Index .I...   \n",
       "2           2  \"It's a huge black eye,\" said publisher Arthur...   \n",
       "3           3  SEC Chairman William Donaldson said there is a...   \n",
       "4           4  Vivendi shares closed 1.9 percent at 15.80 eur...   \n",
       "5           5  Myanmar's pro-democracy leader Aung San Suu Ky...   \n",
       "6           6  Darren Dopp, a Spitzer spokesman, declined to ...   \n",
       "7           7  Bremer said one initiative is to launch a US$7...   \n",
       "8           8  In afternoon trading in Europe, France's CAC-4...   \n",
       "9           9  He later learned that the incident was caused ...   \n",
       "\n",
       "                                              sent_2  score  \n",
       "0  He said the problem needs to be corrected befo...   0.88  \n",
       "1  The broad Standard & Poor's 500 Index .SPX inc...   0.16  \n",
       "2  \"It's a huge black eye,\" Arthur Sulzberger, th...   0.72  \n",
       "3  \"I think there's a building confidence that th...   0.68  \n",
       "4  In New York, Vivendi shares were 1.4 percent d...   0.28  \n",
       "5  Myanmar's pro-democracy leader Aung San Suu Ky...   0.92  \n",
       "6  John Heine, a spokesman for the commission in ...   0.28  \n",
       "7  Bremer said he would launch a $70-million prog...   0.72  \n",
       "8  In Europe, France's CAC-40 rose 1.3 percent, B...   0.40  \n",
       "9  He later found out the alarming incident had b...   1.00  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/STS_Data_12.csv\",index_col=None)\n",
    "data.score = data.score/5.0\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-31ca22b0373b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-72ad431d1c85>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, model, epochs, lr, validation_thresh, batch_size, print_every)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-70d7159086f7>\u001b[0m in \u001b[0;36msemantic_similarity\u001b[0;34m(self, documents_1, documents_2)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mpair_B\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs_2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_similarity_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_B\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_similarity_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_B\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m#averaging score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-70d7159086f7>\u001b[0m in \u001b[0;36m_similarity_tag\u001b[0;34m(self, tag_a, tag_b)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_similarity_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtag_a_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_b_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tag_correlation_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag_a_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag_b_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "model = TextSimilarity()\n",
    "thresh = int(0.8 * len(data))\n",
    "\n",
    "train_data = data.iloc[:thresh]\n",
    "test_data = data.iloc[thresh:]\n",
    "\n",
    "epoch = 100\n",
    "lr = 0.01\n",
    "train(train_data, model, epochs=epoch, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._tag_correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-250088f2061f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemantic_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-8a882509066d>\u001b[0m in \u001b[0;36msemantic_similarity\u001b[0;34m(self, documents_1, documents_2)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mpair_B\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs_2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_similarity_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_B\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_similarity_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_B\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m#averaging score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8a882509066d>\u001b[0m in \u001b[0;36m_similarity_tag\u001b[0;34m(self, tag_a, tag_b)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_similarity_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtag_a_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_b_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tag_correlation_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag_a_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag_b_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "test_Y = test_data.score\n",
    "scores = model.semantic_similarity(test_data.sent_1, test_data.sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
